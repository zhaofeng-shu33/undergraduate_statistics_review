\section{Multivariate Derivative}
Consider a multivariate function $f: \mathbb{R}^n \to \mathbb{R}$. It maps $\b{x} = (x_1, \dots, x_n) $ to a real number.
The gradient of $f$ is a natural extension of derivative for a single variable function. It is defined as an n-dimensional vector function.
$\nabla f: \mathbb{R}^n \to \mathbb{R}^n $
\begin{equation}
\nabla f = (\pdv{f}{x_1}, \dots, \pdv{f}{x_n})
\end{equation}
For a continuous differentiable function $f$ we have 
\begin{equation}\label{eq:differential}
f(x + \b{h} ) = f(x) + \nabla f \cdot \b{h} + o(\norm{\b{h}}_2)
\end{equation}
Consider a unit vector $\b{n}$ and a real positive number $t$, for a fixed $\b{x}_0$ we try to find which direction $\b{n}$ makes $f(\b{x}_0 + t \b{n})$ decreases fastly. To describe the problem mathematically, the optimal $\b{n}$ should satisfy the following condition:
\begin{equation}\label{eq:decrease_fastest}
\forall \norm{\b{v}}_2=1, \exists \epsilon,  \, s.t. f(\b{x}_0 + t \b{n}) \leq f(\b{x}_0 + t \b{v}) \,\forall\, 0 < t < \epsilon
\end{equation}
The concept of directional derivative arises from this problem. For any directional vector $\b{n}$, the directional derivative $\pdv{f}{\b{n}}$ satisfies 

\begin{equation}\label{eq:dd}
f(\b{x}_0 + t \b{n}) = f(\b{x}_0) + 
 \pdv{f}{\b{n}} t + o(t)
 \end{equation}
 
Comparing \eqref{eq:dd} with \eqref{eq:differential} gives $\pdv{f}{\b{n}} = \nabla f \cdot \b{n}$.  Therefore, \eqref{eq:decrease_fastest} can be simplified to 
\begin{equation}\label{eq:find_n}
\nabla \cdot \b{n} \leq \nabla f \cdot{v}, \, \forall \norm{\b{v}}_2=1
\end{equation}
From Cachy's Inequality, $ \nabla f \cdot (-\b{v}) \leq \norm{\nabla f}_2 \norm{\b{v}}_2 = \nabla f \cdot \frac{\nabla f}{\norm{\nabla f}_2}$. Taking $\b{n} = -  \frac{\nabla f}{\norm{\nabla f}_2} $ makes \eqref{eq:find_n} hold.
$\b{n}$ is the unit vector which has the  opposite direction from $\nabla f $. Many function minimization algorithms are based on this property.

\begin{exercise}
Consider function $f: \Theta \to \mathbb{R}$, where $\Theta = \{(r, \theta, \phi)| r>0, 0<\theta<\pi, 0<\phi < 2\pi\}$. The specific form of $f$ is $f(r, \theta,\phi) = r \sin \theta \cos\phi$. Try to calculate the gradient of $f$.
\end{exercise}

In the above, we have discussed the derivative for the scalar function. For vector valued function, the derivative is a matrix instead of a vector and the derivative matrix has another commonly used name called Jacobian matrix.

To write the Jacobian matrix in explicit form, consider $f: \mathbb{R}^n \to \mathbb{R}^m$. As shown by the following equation, the Jacobian matrix of $f$ is an $m\times n$ matrix whose $(i,j)$-th entry is $\pdv{f_i}{x_j}$.
\begin{equation}
J_f = \begin{pmatrix}
\pdv{f_1}{x_1} & \dots & \pdv{f_1}{x_n} \\
\vdots & \ddots & \vdots \\
\pdv{f_m}{x_1} & \dots & \pdv{f_m}{x_n}
\end{pmatrix}
\end{equation}
When $m=n$, $J_f$ is a square matrix and its determinant exists.
The determinant of Jacobian matrix is used when making a change of variable in calculating multiple integrals.
\begin{example}
To calculate	$\iiint_{x^2 + y^2 +z^2 \leq 1} dxdydz$, we make the following variable transformation.
\begin{align*}
x & = r \sin \theta \cos \phi \\
y & = r \sin \theta \sin \phi \\
z & = r \cos \theta
\end{align*}
The domain of $(r, \theta, \phi)$ is described by $\Theta =  \{(r, \theta, \phi)| r \geq 0, 0\leq \theta \leq \pi, 0\leq \phi \leq 2\pi\}$. We can calculate $J_f = r^2 \sin \theta $. In this case, the integral is transformed to $\iiint_{(r, \theta, \phi) \in \Theta} r^2 \sin \theta drd\theta d\phi$, which equals $\frac{4}{3} \pi $.
\end{example}
The concept of derivative can be generalized further to function of matrix. Let $f: \mathbb{R}^{m \times n} \to \mathbb{R}$. We can use the notation $\nabla_{
	\b{A}} f(\b{A}) $ to denote the derivative, which is a matrix of the same size with $\b{A}$:
$(\nabla_{\b{A}} f(\b{A}))_{ij} = \pdv{f(\b{A})}{\b{A}_{ij}}$. For example, the matrix determinant can be regarded as the function of matrix.