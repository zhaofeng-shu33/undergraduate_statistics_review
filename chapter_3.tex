\section{Multivariate Derivative}
Consider a multivariate function $f: \mathbb{R}^n \to \mathbb{R}$. It maps $\b{x} = (x_1, \dots, x_n) $ to a real number.
The gradient of $f$ is a natural extension of derivative for a single variable function. It is defined as an n-dimensional vector function.
$\nabla f: \mathbb{R}^n \to \mathbb{R}^n $
\begin{equation}
\nabla f = (\pdv{f}{x_1}, \dots, \pdv{f}{x_n})
\end{equation}
For a continuous differentiable function $f$ we have 
\begin{equation}\label{eq:differential}
f(x + \b{h} ) = f(x) + \nabla f \cdot \b{h} + o(\norm{\b{h}}_2)
\end{equation}
Consider a unit vector $\b{n}$ and a real positive number $t$, for a fixed $\b{x}_0$ we try to find which direction $\b{n}$ makes $f(\b{x}_0 + t \b{n})$ decreases fastly. To describe the problem mathematically, the optimal $\b{n}$ should satisfy the following condition:
\begin{equation}\label{eq:decrease_fastest}
\forall \norm{\b{v}}_2=1, \exists \epsilon,  \, s.t. f(\b{x}_0 + t \b{n}) \leq f(\b{x}_0 + t \b{v}) \,\forall\, 0 < t < \epsilon
\end{equation}
The concept of directional derivative arises from this problem. For any directional vector $\b{n}$, the directional derivative $\pdv{f}{\b{n}}$ satisfies 

\begin{equation}\label{eq:dd}
f(\b{x}_0 + t \b{n}) = f(\b{x}_0) + 
 \pdv{f}{\b{n}} t + o(t)
 \end{equation}
 
Comparing \eqref{eq:dd} with \eqref{eq:differential} gives $\pdv{f}{\b{n}} = \nabla f \cdot \b{n}$.  Therefore, \eqref{eq:decrease_fastest} can be simplified to 
\begin{equation}\label{eq:find_n}
\nabla \cdot \b{n} \leq \nabla f \cdot{v}, \, \forall \norm{\b{v}}_2=1
\end{equation}
From Cachy's Inequality, $ \nabla f \cdot (-\b{v}) \leq \norm{\nabla f}_2 \norm{\b{v}}_2 = \nabla f \cdot \frac{\nabla f}{\norm{\nabla f}_2}$. Taking $\b{n} = -  \frac{\nabla f}{\norm{\nabla f}_2} $ makes \eqref{eq:find_n} hold.
$\b{n}$ is the unit vector which has the  opposite direction from $\nabla f $. Many function minimization algorithms are based on this property.

\begin{exercise}
Consider function $f: \Theta \to \mathbb{R}$, where $\Theta = \{(r, \theta, \phi)| r>0, 0<\theta<\pi, 0<\phi < 2\pi\}$. The specific form of $f$ is $f(r, \theta,\phi) = r \sin \theta \cos\phi$. Try to calculate the gradient of $f$.
\end{exercise}

In the above, we have discussed the derivative for the scalar function. For vector valued function, the derivative is a matrix instead of a vector and the derivative matrix has another commonly used name called Jacobian matrix.

To write the Jacobian matrix in explicit form, consider $f: \mathbb{R}^n \to \mathbb{R}^m$. As shown by the following equation, the Jacobian matrix of $f$ is an $m\times n$ matrix whose $(i,j)$-th entry is $\pdv{f_i}{x_j}$.
\begin{equation}
J_f = \begin{pmatrix}
\pdv{f_1}{x_1} & \dots & \pdv{f_1}{x_n} \\
\vdots & \ddots & \vdots \\
\pdv{f_m}{x_1} & \dots & \pdv{f_m}{x_n}
\end{pmatrix}
\end{equation}
When $m=n$, $J_f$ is a square matrix and its determinant exists.
The determinant of Jacobian matrix is used when making a change of variable in calculating multiple integrals.
\begin{example}
To calculate	$\iiint_{x^2 + y^2 +z^2 \leq 1} dxdydz$, we make the following variable transformation.
\begin{align*}
x & = r \sin \theta \cos \phi \\
y & = r \sin \theta \sin \phi \\
z & = r \cos \theta
\end{align*}
The domain of $(r, \theta, \phi)$ is described by $\Theta =  \{(r, \theta, \phi)| r \geq 0, 0\leq \theta \leq \pi, 0\leq \phi \leq 2\pi\}$. We can calculate $J_f = r^2 \sin \theta $. In this case, the integral is transformed to $\iiint_{(r, \theta, \phi) \in \Theta} r^2 \sin \theta drd\theta d\phi$, which equals $\frac{4}{3} \pi $.
\end{example}
The concept of derivative can be generalized further to function of matrix. Let $f: \mathbb{R}^{m \times n} \to \mathbb{R}$. We can use the notation $\nabla_{
	\b{A}} f(\b{A}) $ to denote the derivative, which is a matrix of the same size with $\b{A}$:
$(\nabla_{\b{A}} f(\b{A}))_{ij} = \pdv{f(\b{A})}{\b{A}_{ij}}$. For example, the matrix determinant can be regarded as the function of matrix.

\section{Matrix Diagonalization}
For a $n\times n$ matrix $\b{A}$ and column vector $\b{x}$, if $\b{A} \b{x} = \lambda \b{x}$ we call $\lambda$ the eigenvalue and $\b{x}$ the eigenvector of $\b{A}$. For a given eigenvalue, we can show all the eigenvectors form a subspace of $\mathbb{R}^n$.  We call this subspace eigenspace associated with $\lambda$, denoted as $V_{\lambda}=\{\b{x} \in \mathbb{R}^n | \b{A}\b{x} = \lambda \b{x} \}$.

We have introduced how to diagonalize a symmetric matrix in previous part. For general matrix, the diagonalization is not always possible. 
To diagonalize matrix $A$ means writing $\b{A} = \b{B} \b{\Lambda} \b{B}^{-1}$ where $\b{\Lambda}$ is a diagonal matrix. $\b{\Lambda}$ is actually the eigenvalues of $\b{A}$. Consider
$\b{A}=\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}$, all its eigenvalues are zero. If $\b{A} = \b{B} \b{\Lambda} \b{B}^{-1}$ holds, then $\b{\Lambda} = \diag\{0,0\}$ which is a contradiction.

$\b{A} = \b{B} \b{\Lambda} \b{B}^{-1}$ is restrictive, if we consider $\b{A} = \b{B} \b{\Lambda} \b{C}$, the diagonalization is always possible even for non-square matrix. This is called singular value decomposition.
\begin{theorem}
	For an $m\times n$ matrix, there exists an $m\times m$ orthogonal matrix $\b{B}$ and an $n \times n$ orthogonal matrix $\b{C}$ such that $\b{A} = \b{B} \b{\Lambda} \b{C}^\t$ where $\b{\Lambda}$ is an $m \times n$ rectangular diagonal matrix.
\end{theorem}
If $\b{A} = \b{B} \b{\Lambda} \b{C}$, we can get $\b{A}\b{A}^\t = \b{B} \b{\Lambda}\b{\Lambda}^\t \b{B}^\t$ where $\b{\Lambda}\b{\Lambda}^\t$ is a diagonal matrix. That is, columns of $\b{B}$ are the eigenvectors of $\b{A}\b{A}^\t$. Similarly, from $\b{A}^\t \b{A} = \b{C} \b{\Lambda}^\t \b{\Lambda} \b{C}^\t$, we conclude that columns of $\b{C}$ are eigenvectors of $\b{A}^\t \b{A}$.

Since there are columns and rows which are all zeros. we can write the decomposition in a compact form. Let $r = \textrm{rank}(\b{A})$ then $\b{A}$ has $r$ non-zero singular values. $\Lambda_r = \diag\{\sigma_1, \dots, \sigma_r\}$. Let $\b{B}_r, \b{C}_r$ be the selection of first $r$ columns of corresponding matrix. Then $\b{A} = \b{B}_r \b{\Lambda}_r \b{C}_r^\t$ is the compact form of SVD.