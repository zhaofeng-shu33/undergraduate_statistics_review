\section{Square symmetric matrix}
In the last chapter, we introduce that a matrix transforms a vector into another vector. Generally, the transformation is useful when the matrix preserves the inner product. To be more specific, consider $ n \times n $ square matrix $\b{U}$. If $ \b{x} \cdot \b{y} =  \b{U}\b{x} \cdot \b{U}\b{y}$ for any $\b{x}, \b{y} \in \mathbb{R}^n$, we call the transformation preserves the inner product.

To explain this property, first recall the definition of inverse matrix, matrix transpose, diagonal matrix and unit matrix for square matrix. If the transformation $\b{U}$ preserves the inner product, 
we call it the orthogonal matrix which satisfies $\b{U}\b{U}^\t = \b{U}^\t\b{U}= \b{I}$. The columns or rows of an orthogonal matrix forms a basis in $\mathbb{R}^n$.

Let $\b{A}$ be an n-dimensional square symmetric matrix. That is, $\b{A}$ has n rows and n columns. $\b{A}_{ij} = \b{A}_{ji}$. A fundamental theorem says there exists an orthogonal matrix $\b{B}$ such that 
$\b{A} = \b{B} \b{\Lambda} \b{B}^\t$ where $\b{\Lambda}$ is a diagonal matrix. Let $ \b{\Lambda} = \diag\{ \sigma_1, \dots, \sigma_n \}, \b{B} =  (\b{b}_1, \dots, \b{b}_n)$. We can write $\b{A} \b{b}_i = \sigma_i\b{b}_i $ for $i=1, \dots, n $. We call $\sigma_i$ the eigenvalue of the square matrix $\b{A}$ and $\b{b}_i$ the corresponding eigenvector. If all $\sigma_i > 0$, the symetric matrix $\b{A}$ is positive definite. In such case, we have $\b{x}^\t \b{A} \b{x} > 0$ for all $\b{x} \in \mathbb{R}^n$. For a positive definite matrix $\b{A}$, we can find a matrix $\b{L}$ such that $\b{A} = \b{L}\b{L}^\t $.

Recall the concept of determinant of square matrix. We use $\abs{\b{A}}$ to denote the determinant of the matrix $\b{A}$.
One important property is that $\abs{\b{A}\b{B}} = \abs{\b{A}}\abs{\b{B}}$.


\section{random vectors}
Random vectors are collection of 1-dimensional random variables. Let $\bm{X} = (X_1, \dots, X_n)$ 
represent a random vector whose probability measure are in $\mathbb{R}^n$.
That is, there exists a cumulative distribution function (cdf) $F_{\b{X}}: \mathbb{R}^n \rightarrow [0,1]$ 
such that $F_{\bm{X}}(\b{x}) = \Pr(X_1 \leq x_1, \dots, X_n \leq x_n)$ where $ \b{x} = (x_1, \dots, x_n)$.
Here we use italic bold face uppercase Latin letter to represent random vector and normal uppercase Latin letter to denote random variable. We can omit the subscript $X$ if no disambiguity is made. For continuous random vector, $F_{\bm{X}}$ is differential in $\mathbb{R}^n$. Some form of its derivative is called probability density function (pdf) $f_{\bm{X}}(\b{x})$, expressed commonly in integral form as
\begin{equation}
F_{\bm{X}}(\b{x}) = \int_{-\infty}^{x_1}\dots\int_{-\infty}^{x_n} f_{\bm{X}}(\b{x}) dx_1 \dots dx_n
\end{equation}
We can get cdf and pdf for the single random variable $X_i$ from the multivariate function. That is
$ F_{X_i}(x_i) = \displaystyle\lim_{x_j \to +\infty, j\neq i} F_{\bm{X}}(\b{x})$ and 
$ f_{X_1}(x_1) = \iint f_{\bm{X}}(\b{x})dx_2 \dots d x_n$ (use $i=1$ for example).
$F_{X_i}(x_i)$ is called the marginal distribution and $f_{X_i}(x_i)$ is called the marginal density.

\begin{example}\label{ex:g2}
	Let $(X_1, X_2)$ be 2-variate normally distributed, then the pdf has the following form
\begin{equation}
f(x_1, x_2) = \frac{1}{2\pi \sigma_a \sigma_b \sqrt{1-r^2}} \exp(-\frac{1}{2(1-r^2)} \left(\frac{(x_1-a)^2}{\sigma_a^2} + \frac{(x_2-b)^2}{\sigma_b^2} -\frac{2r(x_1-a)(x_2-b)}{\sigma_a \sigma_b}\right)) 
\end{equation}
Let we compute the marginal density for $X_1$. Indeed by using the techniques in double integral, we have
\begin{equation}
f(x_1) = \frac{1}{\sqrt{2\pi}\sigma_a} \exp(-\frac{(x_1-a)^2}{2\sigma_a^2})
\end{equation}

\end{example}
Let $g(\cdot): \mathbb{R}^n \to \mathbb{R}^n$ be an invertible function and $\bm{y} = g(\bm{x})$ be the transformation of $\bm{x}$.
If the pdf of $\bm{x}$ is $f(\b{x})$, we can get the pdf of $\bm{y}$ is $f(g^{-1}(\b{y})) \abs{(g^{-1})'(\b{y})}$. Notice the derivate is actually the Jacobian matrix of $g^{-1}$, whose $(i,j)$-th entry is $\frac{\partial (g^{-1})_i}{\partial y_j}$.

For a random vector, it has the expectation vector $\E[\bm{X}] = (\E[X_1], \dots, \E[X_n])$ and covariance matrix $\b{\Sigma} $ whose $(i,j)$th entry is $\Cov[X_i, X_j]$. Written in matrix form:
$ \b{\Sigma} = \E[\bm{X}\bm{X}^T] - \E[\bm{X}] \E[\bm{X}]^T $.

\section{Multivariate Gaussian Distribution}
Example \ref{ex:g2} shows the case of multivariate Gaussian distribution when the dimension is 2. Generally, in n dimension, given a vector $\bm{\mu}$ and positive definite matrix $\b{\Sigma}$, the pdf of Gaussian distribution has the following form:
\begin{equation}
f(\b{x}; \bm{\mu}, \b{\Sigma}) = \frac{1}{(2\pi)^{n/2} \abs{\b{\Sigma}}^{1/2}} \exp \left(-\frac{1}{2} (\b{x} - \bm{\mu})^\t \b{\Sigma}^{-1} (\b{x} - \bm{\mu})\right)
\end{equation}
We can show that the integral of $f$ over $\mathbb{R}^n$ is indeed 1, thus satisfying the normalization constraint. Indeed, assume $\b{\Sigma}^{-1} = \b{L}^\t \b{L}$ and let $\b{y} = \b{L} (\b{x} - \bm{\mu})$. The integral reduces to
$$
\prod_{i=1}^n \left(\int_{\mathbb{R}}\frac{1}{\sqrt{2\pi}} \exp(-\frac{y^2_i}{2}) dy_i\right) = 1
$$

Let $\bm{y} = \b{L} (\bm{x} - \bm{\mu})$ be the transformation of random vector $\bm{x}$, we can show that $\bm{y}_i \sim N(0, 1)$ and $\bm{y}$ has independent components.  

We can show that for a multivariate Gaussian distribution, $\bm{\mu}$ is the expectation vector and $\b{\Sigma}$ is the covariance matrix.